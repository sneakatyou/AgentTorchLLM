{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#large-population-models","title":"Large Population Models","text":"<p> making complexity simple   differentiable learning over millions of autonomous agents </p> <p> </p> <p>Large Population Models (LPMs) are grounded in state-of-the-art AI research, a summary of which can be found here.</p> <p>AgentTorch LPMs have four key features:</p> <ul> <li>Scalability: AgentTorch models can simulate country-size populations in   seconds on commodity hardware.</li> <li>Differentiability: AgentTorch models can differentiate through simulations   with stochastic dynamics and conditional interventions, enabling   gradient-based learning.</li> <li>Composition: AgentTorch models can compose with deep neural networks (eg:   LLMs), mechanistic simulators (eg: mitsuba) or other LPMs. This helps describe   agent behavior, calibrate simulation parameters and specify expressive   interaction rules.</li> <li>Generalization: AgentTorch helps simulate diverse ecosystems - humans in   geospatial worlds, cells in anatomical worlds, autonomous avatars in digital   worlds.</li> </ul> <p>AgentTorch is building the future of decision engines - inside the body, around us and beyond!</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the framework using <code>pip</code>, like so:</p> <pre><code>&gt; pip install git+https://github.com/agenttorch/agenttorch\n</code></pre> <p>Some models require extra dependencies that have to be installed separately. For more information regarding this, as well as the hardware the project has been run on, please see <code>install.md</code>.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The following section depicts the usage of existing models and population data to run simulations on your machine. It also acts as a showcase of the Agent Torch API.</p> <p>A Jupyter Notebook containing the below examples can be found here.</p>"},{"location":"#executing-a-simulation","title":"Executing a Simulation","text":"<pre><code># re-use existing models and population data easily\nfrom AgentTorch.models import disease\nfrom AgentTorch.populations import new_zealand\n\n# use the executor to plug-n-play\nfrom AgentTorch.execute import Executor\n\nsimulation = Executor(disease, new_zealand)\nsimulation.execute()\n</code></pre>"},{"location":"#using-gradient-based-learning","title":"Using Gradient-Based Learning","text":"<pre><code># agent_\"torch\" works seamlessly with the pytorch API\nfrom torch.optim import SGD\n\n# create the simulation\n# ...\n\n# create an optimizer for the learnable parameters\n# in the simulation\noptimizer = SGD(simulation.parameters())\n\n# learn from each \"episode\" and run the next one\n# with optimized parameters\nfor i in range(episodes):\n    optimizer.zero_grad()\n\n    simulation.execute()\n    optimizer.step()\n    simulation.reset()\n</code></pre>"},{"location":"#talking-to-the-simulation","title":"Talking to the Simulation","text":"<pre><code>from AgentTorch.LLM.qa import SimulationAnalysisAgent, load_state_trace\n\n# create the simulation\n# ...\n\nstate_trace = load_state_trace(simulation)\nanalyzer = SimulationAnalysisAgent(simulation, state_trace)\n\n# ask questions regarding the simulation\nanalyzer.query(\"How are stimulus payments affecting disease?\")\nanalyzer.query(\"Which age group has the lowest median income, and how much is it?\")\n</code></pre>"},{"location":"#guides-and-tutorials","title":"Guides and Tutorials","text":""},{"location":"#understanding-the-framework","title":"Understanding the Framework","text":"<p>A detailed explanation of the architecture of the Agent Torch framework can be found here.</p>"},{"location":"#creating-a-model","title":"Creating a Model","text":"<p>A tutorial on how to create a simple predator-prey model can be found in the <code>tutorials/</code> folder.</p>"},{"location":"#contributing-to-agent-torch","title":"Contributing to Agent Torch","text":"<p>Thank you for your interest in contributing! You can contribute by reporting and fixing bugs in the framework or models, working on new features for the framework, creating new models, or by writing documentation for the project.</p> <p>Take a look at the contributing guide for instructions on how to setup your environment, make changes to the codebase, and contribute them back to the project.</p>"},{"location":"architecture/","title":"Framework Architecture","text":"<p>This document details the architecture of the AgentTorch project, explains all the building blocks involved and points to relevant code implementation and examples.</p> <p>A high-level overview of the AgentTorch Python API is provided by the following block diagram:</p> <p></p> <p>The AgentTorch Python API provides developers with the ability to programmatically create and configure LPMs. This functionality is detailed further in the following sections.</p>"},{"location":"architecture/#runtime","title":"Runtime","text":"<p>The AgentTorch runtime is composed of three essential blocks: the configuration, the registry, and the runner.</p> <p>The configuration holds information about the environment, initial and current state, agents, objects, network metadata, as well as substep definitions. The 'configurator' is defined in <code>config.py</code>.</p> <p>The registry stores all registered substeps, and helper functions, to be called by the runner. It is defined in <code>registry.py</code>.</p> <p>The runner accepts a registry and configuration, and exposes an API to execute all, single or multiple episodes/steps in a simulation. It also maintains the state and trajectory of the simulation across these episodes. It is defined in <code>runner.py</code>, and the substep execution and optimization logic is part of <code>controller.py</code>.</p>"},{"location":"architecture/#data","title":"Data","text":"<p>The data layer is composed of any raw, domain-specific data used by the model (such as agent or object initialization data, environment variables, etc.) as well as the files (YAML or Python code) used to configure the model. An example of domain-specific data for a LPM can be found in the <code>models/covid/data</code> folder. The configuration for the same model can be found in <code>config.yaml</code>.</p>"},{"location":"architecture/#base-classes","title":"Base Classes","text":"<p>The base classes of <code>Agent</code>, <code>Object</code> and <code>Substep</code> form the foundation of the simulation. The agents defined in the configuration learn and interact with either their environment, other agents, or objects through substeps. Substeps are executed in the order of their definition in the configuration, and are split into three parts: <code>SubstepObservation</code>, <code>SubstepAction</code> and <code>SubstepTransition</code>.</p> <ul> <li>A <code>SubstepObservation</code> is defined to observe the state, and pick out those   variables that are of use to the current substep.</li> <li>A <code>SubstepAction</code>, sometimes called a <code>SubstepPolicy</code>, decides the course of   action based on the observations made, and then simulates that action.</li> <li>A <code>SubstepTransition</code> outputs the updates to be made to state variables based   on the action taken in the substep.</li> </ul> <p>An example of a substep will all three parts defined can be found here.</p>"},{"location":"architecture/#domain-extended-classes","title":"Domain Extended Classes","text":"<p>These classes are defined by the developer/user configuring the model, in accordance with the domain of the model. For example, in the COVID model, citizens of the populace are defined as <code>Agents</code>, and <code>Transmission</code> and <code>Quarantine</code> as substeps.</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thanks for your interest in contributing to Agent Torch! This guide will show you how to set up your environment and contribute to this library.</p>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You must have the following software installed:</p> <ol> <li><code>git</code> (latest)</li> <li><code>python</code> (&gt;= 3.10)</li> </ol> <p>Once you have installed the above, follow these instructions to <code>fork</code> and <code>clone</code> the repository (<code>AgentTorch/AgentTorch</code>).</p> <p>Once you have forked and cloned the repository, you can pick out an issue you want to fix/implement!</p>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<p>Once you have cloned the repository to your computer (say, in <code>~/Code/AgentTorch</code>) and picked the issue you want to tackle, create a virtual environment, install all dependencies, and create a new branch to hold all your work.</p> <pre><code># create a virtual environment\n&gt; python -m venv .venv/\n\n# set it up\n&gt; . .venv/bin/activate\n&gt; pip install -r development.txt\n&gt; pip install -e .\n\n# set up the pre commit hooks\n&gt; pre-commit install --config pre-commit.yaml\n\n# create a new branch\n&gt; git switch master\n&gt; git switch --create branch-name\n</code></pre> <p>While naming your branch, make sure the name is short and self explanatory.</p> <p>Once you have created a branch, you can start coding!</p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<p>The project is structured as follows. The comments written next to the file/folder give a brief explanation of what purpose the file/folder serves.</p> <pre><code>.\n\u251c\u2500\u2500 agent_torch/\n\u2502  \u251c\u2500\u2500 helpers/ # defines helper functions used to initialize or work with the state of the simulation.\n\u2502  \u251c\u2500\u2500 llm/ # contains all the code related to using llms as agents in the simulation\n\u2502  \u251c\u2500\u2500 __init__.py # exports everything to the world\n\u2502  \u251c\u2500\u2500 config.py # handles reading and processing the simulation's configuration\n\u2502  \u251c\u2500\u2500 controller.py # executes the substeps for each episode\n\u2502  \u251c\u2500\u2500 initializer.py # creates a simulation from a configuration and registry\n\u2502  \u251c\u2500\u2500 registry.py # registry that stores references to the implementations of the substeps and helper functions\n\u2502  \u251c\u2500\u2500 runner.py # executes the episodes of the simulation, and handles its state\n\u2502  \u251c\u2500\u2500 substep.py # contains base classes for the substep observations, actions and transitions\n\u2502  \u2514\u2500\u2500 utils.py # utility functions used throughout the project\n\u251c\u2500\u2500 docs/\n\u2502  \u251c\u2500\u2500 media/ # assets like screenshots or diagrams inserted in .md files\n\u2502  \u251c\u2500\u2500 tutorials/ # jupyter notebooks with tutorials and their explanations\n\u2502  \u251c\u2500\u2500 architecture.md # the framework's architecture\n\u2502  \u2514\u2500\u2500 install.md # instructions on installing the framework\n\u251c\u2500\u2500 models/\n\u2502  \u251c\u2500\u2500 covid/ # a model simulating disease spread, using the example of covid 19\n\u2502  \u2514\u2500\u2500 predator_prey/ # a simple model used to showcase the features of the framework\n\u251c\u2500\u2500 citation.bib # contains the latex code to use to cite this project\n\u251c\u2500\u2500 contributing.md # this file, helps onboard contributors\n\u251c\u2500\u2500 license.md # contains the license for this project (MIT)\n\u251c\u2500\u2500 readme.md # contains details on the what, why, and how\n\u251c\u2500\u2500 requirements.txt # lists the dependencies of the framework\n\u2514\u2500\u2500 setup.py # defines metadata for the project\n</code></pre> <p>Note that after making any code changes, you should run the <code>black</code> code formatter, as follows:</p> <pre><code>&gt; black agent_torch/ tests/\n</code></pre> <p>You should also ensure all the unit tests pass, especially if you have made changes to any files in the <code>agent_torch/</code> folder.</p> <pre><code>&gt; pytest -vvv tests/\n</code></pre> <p>For any changes to the documentation, run <code>prettier</code> over the <code>*.md</code> files after making changes to them. To preview the generated documentation, run:</p> <pre><code>&gt; mkdocs serve\n</code></pre> <p>Rememeber to add any new pages to the sidebar by editing <code>mkdocs.yaml</code>.</p> <p>If you wish to write a tutorial, write it in a Jupyter Notebook, and then convert it to a markdown file using <code>nbconvert</code>:</p> <pre><code>&gt; pip install nbconvert\n&gt; jupyter nbconvert --to markdown &lt;file&gt;.ipynb\n&gt; mv &lt;file&gt;.md index.md\n</code></pre> <p>Rememeber to move any files that it generates to the <code>docs/media</code> folder, and update the hyperlinks in the generated markdown file.</p>"},{"location":"contributing/#saving-changes","title":"Saving Changes","text":"<p>After you have made changes to the code, you will want to <code>commit</code> (basically, Git's version of save) the changes. To commit the changes you have made locally:</p> <pre><code>&gt; git add this/folder that-file.js\n&gt; git commit --message 'commit-message'\n</code></pre> <p>While writing the <code>commit-message</code>, try to follow the below guidelines:</p> <p>Prefix the message with <code>type:</code>, where <code>type</code> is one of the following dependending on what the commit does:</p> <ul> <li><code>fix</code>: Introduces a bug fix.</li> <li><code>feat</code>: Adds a new feature.</li> <li><code>test</code>: Any change related to tests.</li> <li><code>perf</code>: Any performance related change.</li> <li><code>meta</code>: Any change related to the build process, workflows, issue templates,   etc.</li> <li><code>refc</code>: Any refactoring work.</li> <li><code>docs</code>: Any documentation related changes.</li> </ul> <p>Try to keep the first line brief, and less than 60 characters. Describe the change in detail in a new paragraph (double newline after the first line).</p>"},{"location":"contributing/#contributing-changes","title":"Contributing Changes","text":"<p>Once you have committed your changes, you will want to <code>push</code> (basically, publish your changes to GitHub) your commits. To push your changes to your fork:</p> <pre><code>&gt; git push origin branch-name\n</code></pre> <p>If there are changes made to the <code>master</code> branch of the <code>AgentTorch/AgentTorch</code> repository, you may wish to merge those changes into your branch. To do so, you can run the following commands:</p> <pre><code>&gt; git fetch upstream master\n&gt; git merge upstream/master\n</code></pre> <p>This will automatically add the changes from <code>master</code> branch of the <code>AgentTorch/AgentTorch</code> repository to the current branch. If you encounter any merge conflicts, follow this guide to resolve them.</p> <p>Once you have pushed your changes to your fork, follow these instructions to open a <code>pull request</code>:</p> <p>Once you have submitted a pull request, the maintainers of the repository will review your pull requests and provide feedback. If they find the work to be satisfactory, they will merge the pull request.</p>"},{"location":"contributing/#thanks-for-contributing","title":"Thanks for contributing!","text":""},{"location":"install/","title":"Installation Guide","text":"<p>AgentTorch is meant to be used in a Python 3.9 environment. If you have not installed Python 3.9, please do so first from python.org/downloads.</p> <p>To install the project, run:</p> <pre><code>&gt; pip install git+https://github.com/agenttorch/agenttorch\n</code></pre> <p>To run some models, you may need to separately install their dependencies. These usually include <code>torch</code>, <code>torch_geometric</code>, and <code>osmnx</code>.</p> <p>For the sake of completeness, a summary of the commands required is given below:</p> <pre><code># on macos, cuda is not available:\n&gt; pip install torch torchvision torchaudio\n&gt; pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv\n&gt; pip install osmnx\n\n# on ubuntu, where ${CUDA} is the cuda version:\n&gt; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/${CUDA}\n&gt; pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+${CUDA}.html\n&gt; pip install osmnx\n</code></pre>"},{"location":"install/#hardware","title":"Hardware","text":"<p>The code has been tested on macOS Catalina 10.1.7 and Ubuntu 22.04.2 LTS. Large-scale experiments are run using Nvidia's TitanX and V100 GPUs.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>The following tutorials, in alphabetical order, can be found in this folder:</p> <ul> <li>Creating a model</li> <li>Using models</li> </ul>"},{"location":"tutorials/configure-behavior/","title":"Guide to AI Agent Behavior Generation","text":"<p>Welcome to this comprehensive tutorial on AI agent behavior generation! This guide is designed for newcomers who want to learn how to create AI agents and simulate population behaviors using a custom framework. We'll walk you through each step, explaining concepts as we go.</p>"},{"location":"tutorials/configure-behavior/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to the Framework</li> <li>Setting Up Your Environment</li> <li>Understanding the Core Components</li> <li>Creating Your First AI Agent</li> <li>Generating Population Behaviors</li> <li>Putting It All Together</li> <li>Next Steps and Advanced Topics</li> </ol>"},{"location":"tutorials/configure-behavior/#1-introduction-to-the-framework","title":"1. Introduction to the Framework","text":"<p>Our framework is designed to simulate population behaviors using AI agents. It combines several key components:</p> <ul> <li>LLM Agents: We use Large Language Models (LLMs) to create intelligent   agents that can make decisions based on given scenarios.</li> <li>Archetypes: These represent different types of individuals in a   population.</li> <li>Behaviors: These simulate how individuals might act in various situations.</li> </ul> <p>This framework is particularly useful for modeling complex social or economic scenarios, such as population responses during a pandemic.</p> <p></p>"},{"location":"tutorials/configure-behavior/#2-setting-up-your-environment","title":"2. Setting Up Your Environment","text":"<p>Now, let's set up your OpenAI API key (you'll need an OpenAI account):</p> <pre><code>OPENAI_API_KEY = None # Replace with your actual API key\n</code></pre> <p></p>"},{"location":"tutorials/configure-behavior/#3-understanding-the-core-components","title":"3. Understanding the Core Components","text":"<p>Let's break down the main components of our framework:</p>"},{"location":"tutorials/configure-behavior/#dspyllm-and-langchainllm","title":"DspyLLM and LangchainLLM","text":"<p>These are wrappers around language models that allow us to create AI agents. They can process prompts and generate responses based on given scenarios.</p>"},{"location":"tutorials/configure-behavior/#archetype","title":"Archetype","text":"<p>This component helps create different \"types\" of individuals in our simulated population. Like Male under 19, Female from 20 to 29 years of age.</p>"},{"location":"tutorials/configure-behavior/#behavior","title":"Behavior","text":"<p>The Behavior component simulates how individuals (or groups) might act in various situations. It uses the AI agents to generate these behaviors.</p> <p>Now you have two AI agents ready to process prompts!</p>"},{"location":"tutorials/configure-behavior/#4-creating-llm-agents","title":"4. Creating LLM Agents","text":""},{"location":"tutorials/configure-behavior/#using-dspy","title":"Using DSPy","text":"<pre><code>from dspy_modules import COT, BasicQAWillToWork\nfrom agent_torch.core.llm.llm import DspyLLM\n\nllm_dspy = DspyLLM(qa=BasicQAWillToWork, cot=COT, openai_api_key=OPENAI_API_KEY)\nllm_dspy.initialize_llm()\n\noutput_dspy = llm_dspy.prompt([\"You are an individual living during the COVID-19 pandemic. You need to decide your willingness to work each month and portion of your assets you are willing to spend to meet your consumption demands, based on the current situation of NYC.\"])\nprint(\"DSPy Output:\", output_dspy)\n</code></pre>"},{"location":"tutorials/configure-behavior/#using-langchain","title":"Using Langchain","text":"<pre><code>from agent_torch.core.llm.llm import LangchainLLM\n\nagent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n\nllm_langchian = LangchainLLM(openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\")\nllm_langchian.initialize_llm()\n\noutput_langchain = llm_langchian.prompt([\"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0.0 and 1.0, only.\"])\nprint(\"Langchain Output:\", output_langchain)\n</code></pre>"},{"location":"tutorials/configure-behavior/#5-generating-population-behaviors","title":"5. Generating Population Behaviors","text":"<p>To simulate population behaviors, we'll use the Archetype and Behavior classes:</p> <pre><code>from agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.populations import NYC\n\n# Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=7)\n\n# Define a prompt template\n# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\nuser_prompt_template = \"Your age is {age} {gender}, unemployment rate is {unemployment_rate}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n\n# Create a behavior model\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\nearning_behavior = Behavior(\n    archetype=archetype.llm(llm=llm_dspy, user_prompt=user_prompt_template, num_agents=12),\n    region=NYC\n)\n\nprint(\"Behavior model created successfully!\")\n</code></pre> <p>This sets up a behavior model that can simulate how 12 different agents might behave in NYC during the COVID-19 pandemic.</p> <p></p>"},{"location":"tutorials/configure-behavior/#6-putting-it-all-together","title":"6. Putting It All Together","text":"<p>Now, let's use our behavior model to generate some population behaviors:</p> <pre><code># Define scenario parameters\n# Names of the parameters should match the placeholders in the user_prompt template\nscenario_params = {\n    'month': 'January',\n    'year': '2020',\n    'covid_cases': 1200,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.05,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <pre><code># Define another scenario parameters\nscenario_params = {\n    'month': 'February',\n    'year': '2020',\n    'covid_cases': 900,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.1,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <pre><code># Define yet another scenario parameters\nscenario_params = {\n    'month': 'March',\n    'year': '2020',\n    'covid_cases': 200,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.11,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <p>And so on...</p> <p>This will output a set of behaviors for our simulated population based on the given scenario.</p> <p></p>"},{"location":"tutorials/configure-behavior/#7-next-steps-and-advanced-topics","title":"7. Next Steps and Advanced Topics","text":"<p>You've just created your first AI agents and simulated population behaviors. Here are some advanced topics you might want to explore next:</p> <ul> <li>Customizing archetypes for specific populations</li> <li>Creating more complex behavior models</li> </ul>"},{"location":"tutorials/creating-a-model/","title":"Predator-Prey Model","text":"Imports <pre><code># import agent-torch\n\nimport os\nimport sys\nmodule_path = os.path.abspath(os.path.join('../../../agent_torch'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom AgentTorch import Runner, Registry\nfrom AgentTorch.substep import SubstepObservation, SubstepAction, SubstepTransition\nfrom AgentTorch.helpers import get_by_path, read_config, read_from_file, grid_network\n</code></pre> <pre><code># import all external libraries that we need.\n\nimport math\nimport torch\nimport re\nimport random\nimport argparse\nimport numpy as np\nimport torch.nn as nn\nimport networkx as nx\nimport osmnx as ox\nfrom tqdm import trange\n</code></pre> <pre><code># define the helper functions we need.\n\ndef get_var(state, var):\n  \"\"\"\n    Retrieves a value from the current state of the model.\n  \"\"\"\n  return get_by_path(state, re.split('/', var))\n</code></pre> <p>The complete code for this model can be found here. The architecture of the AgentTorch framework, which explains some key concepts, can be found here.</p> <p>This guide walks you through creating a custom predator-prey model using the AgentTorch framework. This model will simulate an ecosystem consisting of predators, prey and grass: predators eat prey, and prey eat grass.</p> <p>The model's parameters, rules and configuration are passed to AgentTorch, which iteratively simulates the model, allowing you to optimize its learnable parameters, while also modelling the simulation in real time. AgentTorch's Python API is based on PyTorch, which enhances its performance on GPUs.</p> <p>The following sections detail:</p> <ul> <li>an overview of the model's rules and parameters.</li> <li>the properties of all entities stored in the model's state.</li> <li>the substeps that observe, simulate and modify the state for each agent.</li> <li>the code required to run the simulation using <code>agent-torch</code>.</li> <li>plotting the state's trajectory using <code>matplotlib</code>.</li> </ul>"},{"location":"tutorials/creating-a-model/#model-overview","title":"Model Overview","text":"<p>The following are configurable parameters of the model:</p> <ul> <li>a $n \\times m$ grid, with $p$ predators and $q$ prey to start with.</li> <li>grass can grown on any of the $n \\cdot m$ squares in the grid.</li> </ul> <p>The rules followed by the simulated interactions are configured as follows:</p> <ul> <li>predators can eat only prey, and prey can eat only grass.</li> <li>grass grows back once eaten after a certain number of steps.</li> <li>upon consuming food, the energy of the consumer increases.</li> <li>movement happens randomly, to any neighbouring square in the grid.</li> <li>each move reduces the energy of the entity by a fixed amount.</li> </ul> <p>These parameters and rules, along with the properties of the entities (detailed below) in the simulation are defined in a configuration file, and passed on to the model.</p>"},{"location":"tutorials/creating-a-model/#state-environment-agents-and-objects","title":"State: Environment, Agents, and Objects","text":"<p>The model's state consists of a list of properties of the simulated environment, and the agents and objects situated in that simulation. For this model, the:</p>"},{"location":"tutorials/creating-a-model/#environment","title":"Environment","text":"<p>The environment will have only one property: the size of the two-dimensional grid in which the predators and prey wander, defined like so:</p> <pre><code>environment:\n  bounds: (max_x, max_y) # tuple of integers\n</code></pre>"},{"location":"tutorials/creating-a-model/#agents","title":"Agents","text":"<p>This model has two agents: predator, and prey.</p>"},{"location":"tutorials/creating-a-model/#predator","title":"Predator","text":"<p>The predator agent is defined like so:</p> <pre><code>predator:\n  coordinates: (x, y) # tuple of integers\n  energy: float\n  stride_work: float\n</code></pre> <p>The <code>coordinates</code> property depicts the current position of the predator in the two-dimensional grid. It is initialized from a CSV file that contains a list of randomly generated coordinates for all 40 predators.</p> <p>The <code>energy</code> property stores the current amount of energy possessed by the predator. Initially, this property is set to a random number between 30 and 100.</p> <p>The <code>stride_work</code> property is a static, but learnable property that stores the amount of energy to deduct from a predator for one step in any direction on the grid.</p>"},{"location":"tutorials/creating-a-model/#prey","title":"Prey","text":"<p>The prey agent is identical to the predator agent, and has one additional property: <code>nutritional_value</code>.</p> <pre><code>prey:\n  coordinates: (x, y) # tuple of integers\n  energy: float\n  stride_work: float\n  nutritional_value: float\n</code></pre> <p>The <code>nutritional_value</code> property is a static but learnable property that stores the amount of energy gained by a predator when it consumes a single prey entity.</p>"},{"location":"tutorials/creating-a-model/#objects","title":"Objects","text":"<p>This model has only one agent: grass.</p>"},{"location":"tutorials/creating-a-model/#grass","title":"Grass","text":"<p>The grass entity is defined as follows:</p> <pre><code>grass:\n  coordinates: (x, y)\n  growth_stage: 0|1\n  growth_countdown: float\n  regrowth_time: float\n  nutritional_value: float\n</code></pre> <p>The <code>coordinates</code> property depicts the current position of the predator in the two-dimensional grid. It is initialized from a CSV file that contains a list of all 1600 coordinates.</p> <p>The <code>growth_stage</code> property stores the current growth stage of the grass: 0 means it is growing, and 1 means it is fully grown.</p> <p>The <code>growth_countdown</code> property stores the number of steps after which the grass becomes fully grown. The <code>regrowth_time</code> property is static and learnable, and stores the max value of the countdown property.</p> <p>The <code>nutritional_value</code> property is a static but learnable property that stores the amount of energy gained by a predator when it consumes a single prey entity.</p>"},{"location":"tutorials/creating-a-model/#network","title":"Network","text":"<p>The model makes use of the adjacency matrix of a two-dimensional grid filled with predator and prey to simulate the movement of those entities.</p> <pre><code>network:\n  agent_agent:\n    grid: [predator, prey]\n</code></pre>"},{"location":"tutorials/creating-a-model/#substeps","title":"Substeps","text":"<p>Each substep is a <code>torch.nn.ModuleDict</code> that takes an input state, and produces an updated state as output. A substep consists of three phases:</p> <ol> <li>Observation (retrieving relevant information from the state)</li> <li>Policy/Action (deciding on the course of action as per the observations)</li> <li>Transition (randomizing and updating the state according to the action)</li> </ol> <p>This model consists of four substeps: <code>move</code>, <code>eat_grass</code>, <code>hunt_prey</code>, and <code>grow_grass</code>.</p> Helper functions <pre><code># define all the helper functions we need.\n\ndef get_neighbors(pos, adj_grid, bounds):\n  \"\"\"\n    Returns a list of neighbours for each position passed in the given\n    `pos` tensor, using the adjacency matrix passed in `adj_grid`.\n  \"\"\"\n  x, y = pos\n  max_x, max_y = bounds\n\n  # calculate the node number from the x, y coordinate.\n  # each item (i, j) in the adjacency matrix, if 1 depicts\n  # that i is connected to j and vice versa.\n  node = (max_y * x) + y\n  conn = adj_grid[node]\n\n  neighbors = []\n  for idx, cell in enumerate(conn):\n    # if connected, calculate the (x, y) coords of the other\n    # node and add it to the list of neighbors.\n    if cell == 1:\n      c = (int) (idx % max_y)\n      r = math.floor((idx - c) / max_y)\n\n      neighbors.append(\n        [torch.tensor(r), torch.tensor(c)]\n      )\n\n  return torch.tensor(neighbors)\n\n# define a function to retrieve the input required\ndef get_find_neighbors_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    adj_grid = get_var(state, input_variables['adj_grid'])\n    positions = get_var(state, input_variables['positions'])\n\n    return bounds, adj_grid, positions\n\ndef get_decide_movement_input(state, input_variables):\n    positions = get_var(state, input_variables['positions'])\n    energy = get_var(state, input_variables['energy'])\n\n    return positions, energy\n\ndef get_update_positions_input(state, input_variables):\n    prey_energy = get_var(state, input_variables['prey_energy'])\n    pred_energy = get_var(state, input_variables['pred_energy'])\n    prey_work = get_var(state, input_variables['prey_work'])\n    pred_work = get_var(state, input_variables['pred_work'])\n\n    return prey_energy, pred_energy, prey_work, pred_work\n\ndef get_find_eatable_grass_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    positions = get_var(state, input_variables['positions'])\n    grass_growth = get_var(state, input_variables['grass_growth'])\n\n    return bounds, positions, grass_growth\n\ndef get_eat_grass_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    energy = get_var(state, input_variables['energy'])\n    nutrition = get_var(state, input_variables['nutrition'])\n    grass_growth = get_var(state, input_variables['grass_growth'])\n    growth_countdown = get_var(state, input_variables['growth_countdown'])\n    regrowth_time = get_var(state, input_variables['regrowth_time'])\n\n    return bounds, prey_pos, energy, nutrition, grass_growth, growth_countdown, regrowth_time\n\ndef get_find_targets_input(state, input_variables):\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    pred_pos = get_var(state, input_variables['pred_pos'])\n\n    return prey_pos, pred_pos\n\ndef get_hunt_prey_input(state, input_variables):\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    prey_energy = get_var(state, input_variables['prey_energy'])\n    pred_pos = get_var(state, input_variables['pred_pos'])\n    pred_energy = get_var(state, input_variables['pred_energy'])\n    nutrition = get_var(state, input_variables['nutritional_value'])\n\n    return prey_pos, prey_energy, pred_pos, pred_energy, nutrition\n\ndef get_grow_grass_input(state, input_variables):\n    grass_growth = get_var(state, input_variables['grass_growth'])\n    growth_countdown = get_var(state, input_variables['growth_countdown'])\n\n    return grass_growth, growth_countdown\n</code></pre>"},{"location":"tutorials/creating-a-model/#move","title":"Move","text":"<p>First, we observe the state, and find a list of neighboring positions for each of the predators/prey currently alive.</p> <pre><code>@Registry.register_substep(\"find_neighbors\", \"observation\")\nclass FindNeighbors(SubstepObservation):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state):\n    bounds, adj_grid, positions = get_find_neighbors_input(state, self.input_variables)\n\n    # for each agent (prey/predator), find the adjacent cells and pass\n    # them on to the policy class.\n    possible_neighbors = []\n    for pos in positions:\n      possible_neighbors.append(\n        get_neighbors(pos, adj_grid, bounds)\n      )\n\n    return { self.output_variables[0]: possible_neighbors }\n</code></pre> <p>Then, we decide the course of action: to move each entity to a random neighboring position, only if they have the energy to do so.</p> <pre><code>@Registry.register_substep(\"decide_movement\", \"policy\")\nclass DecideMovement(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    positions, energy = get_decide_movement_input(state, self.input_variables)\n    possible_neighbors = observations['possible_neighbors']\n\n    # randomly choose the next position of the agent. if the agent\n    # has non-positive energy, don't let it move.\n    next_positions = []\n    for idx, pos in enumerate(positions):\n      next_positions.append(\n        random.choice(possible_neighbors[idx]) if energy[idx] &gt; 0 else pos\n      )\n\n    return { self.output_variables[0]: torch.stack(next_positions, dim=0) }\n</code></pre> <p>Lastly, we update the state, with the new positions of the entities, and reduce the energy of each entity by the value of the <code>stride_work</code> learnable parameter.</p> <pre><code>@Registry.register_substep(\"update_positions\", \"transition\")\nclass UpdatePositions(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    prey_energy, pred_energy, prey_work, pred_work = get_update_positions_input(state, self.input_variables)\n\n    # reduce the energy of the agent by the work required by them\n    # to take one step.\n    prey_energy = prey_energy + torch.full(prey_energy.shape, -1 * (prey_work.item()))\n    pred_energy = pred_energy + torch.full(pred_energy.shape, -1 * (pred_work.item()))\n\n    return {\n      self.output_variables[0]: action['prey']['next_positions'],\n      self.output_variables[1]: prey_energy,\n      self.output_variables[2]: action['predator']['next_positions'],\n      self.output_variables[3]: pred_energy\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#eat","title":"Eat","text":"<p>First, decide which grass is fit to be consumed by the prey.</p> <pre><code>@Registry.register_substep(\"find_eatable_grass\", \"policy\")\nclass FindEatableGrass(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    bounds, positions, grass_growth = get_find_eatable_grass_input(state, self.input_variables)\n\n    # if the grass is fully grown, i.e., its growth_stage is equal to\n    # 1, then it can be consumed by prey.\n    eatable_grass_positions = []\n    max_x, max_y = bounds\n    for pos in positions:\n      x, y = pos\n      node = (max_y * x) + y\n      if grass_growth[node] == 1:\n        eatable_grass_positions.append(pos)\n\n    # pass on the consumable grass positions to the transition class.\n    return { self.output_variables[0]: eatable_grass_positions }\n</code></pre> <p>Then, simulate the consumption of the grass, and update the growth stage, growth countdown, and energies of the grass and prey respectively.</p> <pre><code>@Registry.register_substep(\"eat_grass\", \"transition\")\nclass EatGrass(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    bounds, prey_pos, energy, nutrition, grass_growth, growth_countdown, regrowth_time = get_eat_grass_input(state, self.input_variables)\n\n    # if no grass can be eaten, skip modifying the state.\n    if len(action['prey']['eatable_grass_positions']) &lt; 1:\n      return {}\n\n    eatable_grass_positions = torch.stack(action['prey']['eatable_grass_positions'], dim=0)\n    max_x, max_y = bounds\n    energy_mask = None\n    grass_mask, countdown_mask = torch.zeros(*grass_growth.shape), torch.zeros(*growth_countdown.shape)\n\n    # for each consumable grass, figure out if any prey agent is at\n    # that position. if yes, then mark that position in the mask as\n    # true. also, for all the grass that will be consumed, reset the\n    # growth stage.\n    for pos in eatable_grass_positions:\n      x, y = pos\n      node = (max_y * x) + y\n\n      # TODO: make sure dead prey cannot eat\n      e_m = (pos == prey_pos).all(dim=1).view(-1, 1)\n      if energy_mask is None:\n        energy_mask = e_m\n      else:\n        energy_mask = e_m + energy_mask\n\n      grass_mask[node] = -1\n      countdown_mask[node] = regrowth_time - growth_countdown[node]\n\n    # energy + nutrition adds the `nutrition` tensor to all elements in\n    # the energy tensor. the (~energy_mask) ensures that the change is\n    # undone for those prey that did not consume grass.\n    energy = energy_mask*(energy + nutrition) + (~energy_mask)*energy\n\n    # these masks use simple addition to make changes to the original\n    # values of the tensors.\n    grass_growth = grass_growth + grass_mask\n    growth_countdown = growth_countdown + countdown_mask\n\n    return {\n      self.output_variables[0]: energy,\n      self.output_variables[1]: grass_growth,\n      self.output_variables[2]: growth_countdown\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#hunt","title":"Hunt","text":"<p>First, decide which prey are to be eaten.</p> <pre><code>@Registry.register_substep(\"find_targets\", \"policy\")\nclass FindTargets(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    prey_pos, pred_pos = get_find_targets_input(state, self.input_variables)\n\n    # if there are any prey at the same position as a predator,\n    # add them to the list of targets to kill.\n    target_positions = []\n    for pos in pred_pos:\n      if (pos == prey_pos).all(-1).any(-1) == True:\n        target_positions.append(pos)\n\n    # pass that list of targets to the transition class.\n    return { self.output_variables[0]: target_positions }\n</code></pre> <p>Then, update the energies of both the prey and the predator.</p> <pre><code>@Registry.register_substep(\"hunt_prey\", \"transition\")\nclass HuntPrey(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    prey_pos, prey_energy, pred_pos, pred_energy, nutrition = get_hunt_prey_input(state, self.input_variables)\n\n    # if there are no targets, skip the state modifications.\n    if len(action['predator']['target_positions']) &lt; 1:\n      return {}\n\n    target_positions = torch.stack(action['predator']['target_positions'], dim=0)\n\n    # these are masks similars to the ones in `substeps/eat.py`.\n    prey_energy_mask = None\n    pred_energy_mask = None\n    for pos in target_positions:\n      pye_m = (pos == prey_pos).all(dim=1).view(-1, 1)\n      if prey_energy_mask is None:\n        prey_energy_mask = pye_m\n      else:\n        prey_energy_mask = prey_energy_mask + pye_m\n\n      pde_m = (pos == pred_pos).all(dim=1).view(-1, 1)\n      if pred_energy_mask is None:\n        pred_energy_mask = pde_m\n      else:\n        pred_energy_mask = pred_energy_mask + pde_m\n\n    # any prey that is marked for death should be given zero energy.\n    prey_energy = prey_energy_mask*0 + (~prey_energy_mask)*prey_energy\n    # any predator that has hunted should be given additional energy.\n    pred_energy = pred_energy_mask*(pred_energy + nutrition) + (~pred_energy_mask)*pred_energy\n\n    return {\n      self.output_variables[0]: prey_energy,\n      self.output_variables[1]: pred_energy\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#grow","title":"Grow","text":"<p>In this substep, we simply update the growth countdown of every grass object, and if the countdown has elapsed, we update the growth stage to <code>1</code>.</p> <pre><code>@Registry.register_substep(\"grow_grass\", \"transition\")\nclass GrowGrass(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    grass_growth, growth_countdown = get_grow_grass_input(state, self.input_variables)\n\n    # reduce all countdowns by 1 unit of time.\n    growth_countdown_mask = torch.full(growth_countdown.shape, -1)\n    growth_countdown = growth_countdown + growth_countdown_mask\n\n    # if the countdown has reached zero, set the growth stage to 1,\n    # otherwise, keep it zero.\n    grass_growth_mask = (growth_countdown &lt;= 0).all(dim=1)\n    grass_growth = grass_growth_mask*(1) + (~grass_growth_mask)*(0)\n\n    return {\n      self.output_variables[0]: grass_growth.view(-1, 1),\n      self.output_variables[1]: growth_countdown\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#execution-configuration-registry-and-runner","title":"Execution: Configuration, Registry, and Runner","text":""},{"location":"tutorials/creating-a-model/#configuration","title":"Configuration","text":"<p>There are several parts to the configuration, written in a file traditionally called <code>config.yaml</code>. The following is a brief overview of all the major sections in the configuration file.</p> <pre><code># config.yaml\n# configuration for the predator-prey model.\n\nmetadata:\n  # device type, episode count, data files, etc.\n\nstate:\n  environment:\n    # variables/properties of the simulated enviroment.\n\n  agents:\n    # a list of agents in the simulation, and their properties.\n    # each property must be initialized by specifying a value\n    # or a generator function, and have a fixed tensor shape.\n\n  objects:\n    # a list of objects, similar to the agents list.\n\n  network:\n    # a list of interaction models for the simulation.\n    # could be a grid, or a directed graph, etc.\n\nsubsteps:\n  # a list of substeps\n  # each substep has a list of agents to run that substep for\n  # as well as the function, input and output variables for each\n  # part of that substep (observation, policy and transition)\n</code></pre> <p>The following is an example of defining a property in the configuration.</p> <pre><code>bounds:\n  name: 'Bounds'\n  learnable: false\n  shape: 2\n  dtype: 'int'\n  value:\n    - ${simulation_metadata.max_x} # you can refer to other parts of the config using\n    - ${simulation_metadata.max_y} # the template syntax, i.e., ${path.to.config.value}\n  initialization_function: null\n</code></pre> <p>Notice that to define one single property, we mentioned:</p> <ul> <li>the name of the property, here, <code>'bounds'</code>.</li> <li>whether or not the property is learnable, in this case, <code>false</code>.</li> <li>the shape of the tensor that stores the values, in this case, it is a   one-dimensional array of two elements: <code>(max_x, max_y)</code>.</li> <li>the value of the property, either by directly providing the value or by   providing a function that returns the value.</li> </ul> <p>The full configuration for the predator-prey model can be found here.</p> <pre><code># define helper functions used in the configuration\n\n@Registry.register_helper('map', 'network')\ndef map_network(params):\n  coordinates = (40.78264403323726, -73.96559413265355) # central park\n  distance = 550\n\n  graph = ox.graph_from_point(coordinates, dist=distance, simplify=True, network_type=\"walk\")\n  adjacency_matrix = nx.adjacency_matrix(graph).todense()\n\n  return graph, torch.tensor(adjacency_matrix)\n\n@Registry.register_helper('random_float', 'initialization')\ndef random_float(shape, params):\n  \"\"\"\n    Generates a `Tensor` of the given shape, with random floating point\n    numbers in between and including the lower and upper limit.\n  \"\"\"\n\n  max = params['upper_limit'] + 1 # include max itself.\n  min = params['lower_limit']\n\n  # torch.rand returns a tensor of the given shape, filled with\n  # floating point numbers in the range (0, 1]. multiplying the\n  # tensor by max - min and adding the min value ensure it's\n  # within the given range.\n  tens = (max - min) * torch.rand(shape) + min\n\n  return tens\n\n@Registry.register_helper('random_int', 'initialization')\ndef random_int(shape, params):\n  \"\"\"\n    Generates a `Tensor` of the given shape, with random integers in\n    between and including the lower and upper limit.\n  \"\"\"\n\n  max = math.floor(params['upper_limit'] + 1) # include max itself.\n  min = math.floor(params['lower_limit'])\n\n  # torch.randint returns the tensor we need.\n  tens = torch.randint(min, max, shape)\n\n  return tens\n</code></pre>"},{"location":"tutorials/creating-a-model/#registry-and-runner","title":"Registry and Runner","text":"<p>The code that executes the simulation uses the AgentTorch <code>Registry</code> and <code>Runner</code>, like so:</p> <pre><code>config = read_config('config-map.yaml')\nmetadata = config.get('simulation_metadata')\nnum_episodes = metadata.get('num_episodes')\nnum_steps_per_episode = metadata.get('num_steps_per_episode')\nnum_substeps_per_step = metadata.get('num_substeps_per_step')\n</code></pre> <p>The registry is stores all the classes and functions used by the model, and allows the runner to call them as needed when intializing the simulation and executing the substeps.</p> <pre><code>registry = Registry()\nregistry.register(read_from_file, 'read_from_file', 'initialization')\nregistry.register(grid_network, 'grid', key='network')\n</code></pre> <p>The runner intializes and executes the simulation for us. It also returns:</p> <ul> <li>a list of the learnable parameters, so we can run optimization functions on   them and use the optimized values for the next episode.</li> <li>the trajectory of the state so far, so we can visualize the state using   libraries like <code>matplotlib</code>.</li> </ul> <pre><code>runner = Runner(config, registry)\n</code></pre> <p> The source code for the visualizer used in the following block is given in the next section. </p> <pre><code>runner.init()\n\nfor episode in range(num_episodes):\n  runner.step(num_steps_per_episode)\n\n  final_states = list(filter(\n    lambda x: x['current_substep'] == str(num_substeps_per_step - 1),\n    runner.state_trajectory[-1]\n  ))\n  visualizer = Plot(metadata.get('max_x'), metadata.get('max_y'))\n  visualizer.plot(final_states)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"tutorials/creating-a-model/#visualization","title":"Visualization","text":"<p>You can plot the simulation in different ways. In this notebook, two such methods are demonstrated; the X-Y grid, and the OpenStreetMap plot.</p> <pre><code># display the gifs\n\nfrom IPython.display import HTML\n\nHTML(\"\"\"\n    &lt;table&gt;\n    &lt;tr&gt;&lt;td&gt;\n    &lt;video alt=\"grid\" autoplay&gt;\n        &lt;source src=\"../predator-prey.mp4\" type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n    &lt;/td&gt;&lt;td&gt;\n    &lt;img src=\"../predator-prey.gif\" alt=\"map\" /&gt;\n    &lt;/td&gt;&lt;/tr&gt;\n    &lt;/table&gt;\n\"\"\")\n</code></pre> <pre><code># render the map\n\nfrom IPython.display import display, clear_output\n\nimport time\nimport matplotlib\nimport matplotlib.pyplot as plotter\nimport matplotlib.patches as patcher\nimport contextily as ctx\n\n%matplotlib inline\n\nclass Plot:\n  def __init__(self, max_x, max_y):\n    # intialize the scatterplot\n    self.figure, self.axes = None, None\n    self.prey_scatter, self.pred_scatter = None, None\n    self.max_x, self.max_y = max_x, max_y\n\n    plotter.xlim(0, max_x - 1)\n    plotter.ylim(0, max_y - 1)\n    self.i = 0\n\n  def update(self, state):\n    graph = state['network']['agent_agent']['predator_prey']['graph']\n    self.coords = [(node[1]['x'], node[1]['y']) for node in graph.nodes(data=True)]\n    self.coords.sort(key=lambda x: -(x[0] + x[1]))\n\n    self.figure, self.axes = ox.plot_graph(graph, edge_linewidth=0.3, edge_color='gray', show=False, close=False)\n    ctx.add_basemap(self.axes, crs=graph.graph['crs'], source=ctx.providers.OpenStreetMap.Mapnik)\n    self.axes.set_axis_off()\n\n    # get coordinates of all the entities to show.\n    prey = state['agents']['prey']\n    pred = state['agents']['predator']\n    grass = state['objects']['grass']\n\n    # agar energy &gt; 0 hai... toh zinda ho tum!\n    alive_prey = prey['coordinates'][torch.where(prey['energy'] &gt; 0)[0]]\n    alive_pred = pred['coordinates'][torch.where(pred['energy'] &gt; 0)[0]]\n    # show only fully grown grass, which can be eaten.\n    grown_grass = grass['coordinates'][torch.where(grass['growth_stage'] == 1)[0]]\n\n    alive_prey_x, alive_prey_y = np.array([\n      self.coords[(self.max_y * pos[0]) + pos[1]] for pos in alive_prey\n    ]).T\n    alive_pred_x, alive_pred_y = np.array([\n      self.coords[(self.max_y * pos[0]) + pos[1]] for pos in alive_pred\n    ]).T\n\n    # show prey in dark blue and predators in maroon.\n    self.axes.scatter(alive_prey_x, alive_prey_y, c='#0d52bd', marker='.')\n    self.axes.scatter(alive_pred_x, alive_pred_y, c='#8b0000', marker='.')\n\n    # increment the step count.\n    self.i += 1\n    # show the current step count, and the population counts.\n    self.axes.set_title('Predator-Prey Simulation #' + str(self.i), loc='left')\n    self.axes.legend(handles=[\n      patcher.Patch(color='#fc46aa', label=str(self.i) + ' step'),\n      patcher.Patch(color='#0d52bd', label=str(len(alive_prey)) + ' prey'),\n      patcher.Patch(color='#8b0000', label=str(len(alive_pred)) + ' predators'),\n      # patcher.Patch(color='#d1ffbd', label=str(len(grown_grass)) + ' grass')\n    ])\n\n    display(plotter.gcf())\n    clear_output(wait=True)\n    time.sleep(1)\n\n  def plot(self, states):\n    # plot each state, one-by-one\n    for state in states:\n      self.update(state)\n\n    clear_output(wait=True)\n</code></pre>"},{"location":"tutorials/processing-a-population/","title":"Tutorial: Generating Base Population and Household Data","text":"<p>This tutorial will guide you through the process of generating base population and household data for a specified region using census data. We\u2019ll use a <code>CensusDataLoader</code> class to handle the data processing and generation.</p>"},{"location":"tutorials/processing-a-population/#before-starting","title":"Before Starting","text":"<p>Make sure your <code>population data</code> and <code>household data</code> are in the prescribed format. Names of the column need to be same as shown in the excerpts.</p> <p>Lets see a snapshot of the data</p> <p><code>Population Data</code> is a dictionary containing two pandas DataFrames: '<code>age_gender</code>' and '<code>ethnicity</code>'. Each DataFrame provides demographic information for different areas and regions.</p> <p>The <code>age_gender</code> DataFrame provides a comprehensive breakdown of population data, categorized by area, gender, and age group.</p>"},{"location":"tutorials/processing-a-population/#columns-description","title":"Columns Description","text":"<ul> <li><code>area</code>: Serves as a unique identifier for each geographical area, represented   by a string (e.g., <code>'BK0101'</code>, <code>'SI9593'</code>).</li> <li><code>gender</code>: Indicates the gender of the population segment, with possible values   being <code>'female'</code> or <code>'male'</code>.</li> <li><code>age</code>: Specifies the age group of the population segment, using a string   format such as <code>'20t29'</code> for ages 20 to 29, and <code>'U19'</code> for those under 19   years of age.</li> <li><code>count</code>: Represents the total number of individuals within the specified   gender and age group for a given area.</li> <li><code>region</code>: A two-letter code that identifies the broader region encompassing   the area (e.g., <code>'BK'</code> for Brooklyn, <code>'SI'</code> for Staten Island).</li> </ul>"},{"location":"tutorials/processing-a-population/#example-entry","title":"Example Entry","text":"<p>Here is a sample of the data structure within the <code>age_gender</code> DataFrame:</p> area gender age count region BK0101 female 20t29 3396 BK BK0101 male 20t29 3327 BK <p>This example entry demonstrates the DataFrame's layout and the type of demographic data it contains, highlighting its utility for detailed population studies by age and gender.</p> <p>The <code>ethnicity</code> DataFrame is structured to provide detailed population data, segmented by both geographical areas and ethnic groups.</p>"},{"location":"tutorials/processing-a-population/#columns-description_1","title":"Columns Description","text":"<ul> <li><code>area</code>: A unique identifier assigned to each area, formatted as a string   (e.g., <code>'BK0101'</code>, <code>'SI9593'</code>). This identifier helps in pinpointing specific   locations within the dataset.</li> <li><code>ethnicity</code>: Represents the ethnic group of the population in the specified   area.</li> <li><code>count</code>: Indicates the number of individuals belonging to the specified ethnic   group within the area. This is an integer value representing the population   count.</li> <li><code>region</code>: A two-letter code that signifies the broader region that the area   belongs to (e.g., <code>'BK'</code> for Brooklyn, <code>'SI'</code> for Staten Island).</li> </ul>"},{"location":"tutorials/processing-a-population/#example-entry_1","title":"Example Entry","text":"<p>Below is an example of how the data is presented within the DataFrame:</p> area ethnicity count region BK0101 asian 1464 BK BK0101 black 937 BK <p>This example illustrates the structure and type of data contained within the <code>ethnicity</code> DataFrame, showcasing its potential for detailed demographic studies.</p> <p><code>Household Data</code> contains the following columns:</p> <ul> <li><code>area</code>: Represents a unique identifier for each area.</li> <li><code>people_num</code>: The total number of people within the area.</li> <li><code>children_num</code>: The number of children in the area.</li> <li><code>household_num</code>: The total number of households.</li> <li><code>family_households</code>: Indicates the number of households identified as family   households, highlighting family-based living arrangements.</li> <li><code>nonfamily_households</code>: Represents the number of households that do not fall   under the family households category, including single occupancy and unrelated   individuals living together.</li> <li><code>average_household_size</code>: The average number of individuals per household.</li> </ul> <p>Below is a sample excerpt:</p> area people_num children_num household_num family_households nonfamily_households average_household_size 100100 104 56 418 1 0 2.488038 100200 132 73 549 1 0 2.404372 100300 5 0 10 0 1 5.000000 <p>Now that we have verified our input, we can proceed to next steps!</p>"},{"location":"tutorials/processing-a-population/#step-1-set-up-file-paths","title":"Step 1: Set Up File Paths","text":"<p>First, we need to specify the paths to our data files.</p> <p>Make sure to replace the placeholder paths with the actual paths to your data files.</p> <pre><code># Path to the population data file. Update with the actual file path.\nPOPULATION_DATA_PATH = \"docs/tutorials/processing-a-population/sample_data/NYC/population.pkl\"\n\n# Path to the household data file. Update with the actual file path.\nHOUSEHOLD_DATA_PATH = \"docs/tutorials/processing-a-population/sample_data/NYC/household.pkl\"\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-2-define-age-group-mapping","title":"Step 2: Define Age Group Mapping","text":"<p>We\u2019ll define a mapping for age groups to categorize adults and children in the household data:</p> <pre><code>AGE_GROUP_MAPPING = {\n    \"adult_list\": [\"20t29\", \"30t39\", \"40t49\", \"50t64\", \"65A\"],  # Age ranges for adults\n    \"children_list\": [\"U19\"],  # Age range for children\n}\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-3-load-data","title":"Step 3: Load Data","text":"<p>Now, let\u2019s load the population and household data:</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Load household data\nHOUSEHOLD_DATA = pd.read_pickle(HOUSEHOLD_DATA_PATH)\n\n# Load population data\nBASE_POPULATION_DATA = pd.read_pickle(POPULATION_DATA_PATH)\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-4-set-up-additional-parameters","title":"Step 4: Set Up Additional Parameters","text":"<p>We\u2019ll set up some additional parameters that might be needed for data processing. These are not essential for generating population, but still good to know if you decide to use them in future.</p> <pre><code># Placeholder for area selection criteria, if any. Update or use as needed.\n# Example: area_selector = [\"area1\", \"area2\"]\n# This will be used to filter the population data to only include the selected areas.\narea_selector = None\n\n# Placeholder for geographic mapping data, if any. Update or use as needed.\ngeo_mapping = None\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-5-initialize-the-census-data-loader","title":"Step 5: Initialize the Census Data Loader","text":"<p>Create an instance of the <code>CensusDataLoader</code> class:</p> <pre><code>from agent_torch.data.census.census_loader import CensusDataLoader\n\ncensus_data_loader = CensusDataLoader(n_cpu=8, use_parallel=True)\n</code></pre> <p>This initializes the loader with 8 CPUs and enables parallel processing for faster data generation.</p>"},{"location":"tutorials/processing-a-population/#step-6-generate-base-population-data","title":"Step 6: Generate Base Population Data","text":"<p>Generate the base population data for a specified region:</p> <pre><code>census_data_loader.generate_basepop(\n    input_data=BASE_POPULATION_DATA,  # The population data frame\n    region=\"astoria\",  # The target region for generating base population\n    area_selector=area_selector,  # Area selection criteria, if applicable\n)\n</code></pre> <p>This will create a base population of 100 individuals for the \u201castoria\u201d region. The generated data will be exported to a folder named \u201castoria\u201d under the \u201cpopulations\u201d folder.</p>"},{"location":"tutorials/processing-a-population/#overview-of-the-generated-base-population-data","title":"Overview of the Generated Base Population Data","text":"<p>Each row corresponds to attributes of individual residing in the specified region while generating the population.</p> area age gender ethnicity region BK0101 20t29 female black BK BK0101 20t29 female hispanic BK ... ... ... ... ... BK0101 U19 male asian SI BK0101 U19 female white SI BK0101 U19 male asian SI"},{"location":"tutorials/processing-a-population/#step-7-generate-household-data","title":"Step 7: Generate Household Data","text":"<p>Finally, generate the household data for the specified region:</p> <pre><code>census_data_loader.generate_household(\n    household_data=HOUSEHOLD_DATA,  # The loaded household data\n    household_mapping=AGE_GROUP_MAPPING,  # Mapping of age groups for household composition\n    region=\"astoria\"  # The target region for generating households\n)\n</code></pre> <p>This will create household data for the \u201castoria\u201d region based on the previously generated base population. The generated data will be exported to the same \u201castoria\u201d folder under the \u201cpopulations\u201d folder.</p>"},{"location":"tutorials/processing-a-population/#bonus-generate-population-data-of-specific-size","title":"Bonus: Generate Population Data of Specific Size","text":"<p>For quick experimentation, this may come in handy.</p> <pre><code>census_data_loader.generate_basepop(\n    input_data=BASE_POPULATION_DATA,  # The population data frame\n    region=\"astoria\",  # The target region for generating base population\n    area_selector=area_selector,  # Area selection criteria, if applicable\n    num_individuals = 100 # Saves data for first 100 individuals, from the generated population\n)\n</code></pre>"},{"location":"tutorials/processing-a-population/#bonus-export-population-data","title":"Bonus: Export Population Data","text":"<p>If you have already generated your synthetic population, you just need to export it to \"populations\" folder under the desired \"region\", in order for you to use it with AgentTorch.</p> <pre><code>POPULATION_DATA_PATH = \"/population_data.pickle\"  # Replace with actual path\ncensus_data_loader.export(population_data_path=POPULATION_DATA_PATH,region=\"astoria\")\n</code></pre> <p>In case you want to export data for only few individuals</p> <pre><code>census_data_loader.export(population_data_path=POPULATION_DATA_PATH,region=\"astoria\",num_individuals = 100)\n</code></pre>"},{"location":"tutorials/processing-a-population/#conclusion","title":"Conclusion","text":"<p>You have now successfully generated both base population and household data for the <code>\u201castoria\u201d</code> region. The generated data can be found in the <code>\u201cpopulations/astoria\u201d</code> folder. You can modify the region name, population size, and other parameters to generate data for different scenarios.</p>"},{"location":"tutorials/using-models/","title":"AgentTorch API","text":"<ul> <li>Execute simulations</li> <li>Customize agents</li> <li>Customize populations</li> <li>Engage with results</li> </ul> Imports <pre><code>import pandas as pd\nimport os\n</code></pre>"},{"location":"tutorials/using-models/#simulation-input","title":"Simulation Input","text":"<p><code>Simulation Input</code> : Input to our simulation is a dataset consisting of population generated based on the census data of a specific region. We have currently generated population for New York City and New Zealand.</p> <p>Lets see how the population data looks like for New Zealand</p> <pre><code>NZ_SYNTHETIC_POPULATION_PATH = os.path.join(os.getcwd(), 'simulator_data/census_populations/NZ/synthetic_populations/population_data.pkl')\n\nnz_population = pd.read_pickle(NZ_SYNTHETIC_POPULATION_PATH)\nnz_population.head(5)\n</code></pre> area age gender ethnicity region 0 204200 U19 female Maori Bay of Plenty 1 204200 U19 female Maori Bay of Plenty 2 204200 U19 male European Bay of Plenty 3 204200 U19 female Maori Bay of Plenty 4 204200 U19 female Maori Bay of Plenty <p>The above data shows the population attributes of 5 agents, like 'age', 'gender', 'region where the agent resides', 'ethnicity' and 'area code of residence'. We assign these attributes to each of our simulated agents, to mimic real world population dynamic.</p> <p>We also provide APIs to generate your own synthetic population, though we will not get into the details here.</p>"},{"location":"tutorials/using-models/#run-simulation","title":"Run Simulation","text":"<p><code>Dataloader</code> is the common interface to load, setup or modify the simulation parameters such as Number of Runs, Population Size, Region for which Population data is to be used. It also does the necessary pipelining to setup the flow of simulation.</p> <p>It compiles a config file which is used by the Executor to run the simulation.</p> <pre><code>from AgentTorch.dataloader import DataLoader\n\n# 'populations' is the package where generated population for each region is saved.\n# We are picking New Zealand's (NZ) population for this run.\nfrom populations import NZ\n\n# 'models' is the package where the simulation logic for different use cases are defined.\n# We are picking the macro_economics model for this simulation. It models the population's economic behavior during covid, by simulating a financial market.\nfrom models import macro_economics\nfrom macro_economics.config import NUM_AGENTS\n\n# Initialize the data loader with the preferred model and the region's population.\ndata_loader = DataLoader(model = macro_economics, region = NZ, population_size = NUM_AGENTS)\n\n</code></pre> <pre><code>Config saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\nConfig saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\nConfig saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\n</code></pre> <p>Example 1: Use <code>Executor</code> API to run a simulation</p> <pre><code># Executor is the common interface to run simulation\n# It takes the model and data loader as input and executes the simulation.\nfrom AgentTorch.execute import Executor\n\nagents_sim = Executor(model = macro_economics,data_loader = data_loader)\nagents_sim.execute()\n\n</code></pre> <pre><code>running episode 0...\nSubstep Action: Earning decision\nLLM benchmark expts\nSome Example Prompts:  ['You are female of age U19, living in the Bay of Plenty region. It is September 2020, number of covid cases is 874.0. The price of Essential Goods is 12.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Otago region. It is September 2020, number of covid cases is 874.0. The price of Essential Goods is 12.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Tasman region. It is September 2020, number of covid cases is 874.0. The price of Essential Goods is 12.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ']\nSubstep: Agent Earning!\nSubstep: Agent Consumption\nExecuting Substep: Labor Market\nExecuting Substep: Financial Market\nSubstep Action: Earning decision\nLLM benchmark expts\nSome Example Prompts:  ['You are female of age U19, living in the Bay of Plenty region. It is October 2020, number of covid cases is 701.0. The price of Essential Goods is 12.012574195861816. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Otago region. It is October 2020, number of covid cases is 701.0. The price of Essential Goods is 12.012574195861816. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Tasman region. It is October 2020, number of covid cases is 701.0. The price of Essential Goods is 12.012574195861816. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ']\nSubstep: Agent Earning!\nSubstep: Agent Consumption\nExecuting Substep: Labor Market\nExecuting Substep: Financial Market\nSimulation Ended.\n</code></pre> <p>Example 2: Use <code>DataLoader</code> API to customize the agent - Make LLM an Agent</p> <p>Let's modify the prompt used by our LLM Agent, and re-run the simulation</p> <p>Original Prompt is : \"You are {gender} of age {age}, living in the {region} region. It is {month} {year}, number of covid cases is {covid_cases}. The price of Essential Goods is {price_of_goods}. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? \"</p> <p>We will modify this to remove the information about <code>price of essential goods</code>. This might, help us understand the population/market behaviour when info on inflation is not present.</p> <pre><code>MODIFIED_EARNING_ACTION_PROMPT = \"You are {gender} of age {age}, living in the {region} region. It is {month} {year}, number of covid cases is {covid_cases}. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? \"\ndata_loader.set_config_attribute('EARNING_ACTION_PROMPT', MODIFIED_EARNING_ACTION_PROMPT)\n\nagent_sim_modified = Executor(model = macro_economics,data_loader = data_loader)\n# With this set, we can now run the simulation again using the executor.\nagent_sim_modified.execute()\n</code></pre> <pre><code>Config saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\nresolvers already registered..\n\nrunning episode 0...\nSubstep Action: Earning decision\nLLM benchmark expts\nSome Example Prompts:  ['You are female of age U19, living in the Bay of Plenty region. It is September 2020, number of covid cases is 874.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Otago region. It is September 2020, number of covid cases is 874.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Tasman region. It is September 2020, number of covid cases is 874.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ']\nSubstep: Agent Earning!\nSubstep: Agent Consumption\nExecuting Substep: Labor Market\nExecuting Substep: Financial Market\nSubstep Action: Earning decision\nLLM benchmark expts\nSome Example Prompts:  ['You are female of age U19, living in the Bay of Plenty region. It is October 2020, number of covid cases is 701.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Otago region. It is October 2020, number of covid cases is 701.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are female of age U19, living in the Tasman region. It is October 2020, number of covid cases is 701.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ']\nSubstep: Agent Earning!\nSubstep: Agent Consumption\nExecuting Substep: Labor Market\nExecuting Substep: Financial Market\nSimulation Ended.\n</code></pre>"},{"location":"tutorials/using-models/#visualize-results","title":"Visualize Results","text":"<p>Simulations produce a state trace</p> <p>More info on state trace ....</p> <pre><code>from macro_economics.config import STATE_TRACE_PATH\nfrom AgentTorch.LLM.llm_qa import load_state_trace\n\nstate_trace = load_state_trace(STATE_TRACE_PATH,NZ_NUM_AGENTS)\nstate_trace[0].columns\n</code></pre> <pre><code>Index(['ID', 'age', 'area', 'assets', 'consumption_propensity', 'ethnicity',\n       'gender', 'household', 'monthly_income', 'post_tax_income', 'region',\n       'will_work', 'month', 'year'],\n      dtype='object')\n</code></pre> <p>Example 3: Use <code>Visualizer</code> API to engage the simulation trace for retrospective and prospective analysis</p> <pre><code># Visualizer is the helper class to visualize the output variables of the simulation\n# As we would expect, it takes simulation trace as input\nfrom AgentTorch.visualize import Visualizer\nvisualizer = Visualizer(state_trace)\n</code></pre> <pre><code># We can plot a few variables of interest like Consumption Propensity and Will to Work.\nvisualizer.plot('consumption_propensity')\n</code></pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook <pre><code># Next lets visualize the Aggregate Willingness to work in each region\nvisualizer.plot('will_to_work')\n</code></pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"tutorials/using-models/#talk-with-simulation","title":"Talk with Simulation","text":"<p><code>Q/A</code> : AgentTorch gives you the ability to perform Q/A with the simulation, through its integration of simulation trace and conversation history with th Analysis Agent (LLM Agent).</p>"},{"location":"tutorials/using-models/#configuration","title":"Configuration","text":"<pre><code>from macro_economics.config import OPENAI_API_KEY\nconversations_memory_directory = ROOT_DIR + 'populations/NZ/simulation_memory_output/2020/9'\nstate_trace_path = ROOT_DIR + 'populations/NZ/state_data_dict.pkl'\n</code></pre>"},{"location":"tutorials/using-models/#analyze","title":"Analyze","text":"<p><code>DocumentRetriever</code> : It sets up a vector db containing all the converations intiated with the LLM Agent, during the runtime of the simulation.</p> <pre><code>from AgentTorch.LLM.llm_qa import DocumentRetriever\nconversation_memory_retriever = DocumentRetriever(directory=conversation_memory_directory)\n</code></pre> <pre><code>Batches:   0%|          | 0/16 [00:00&lt;?, ?it/s]\n</code></pre> <p><code>SimulationAnalysisAgent</code> abstracts away the logic for analysis module, so you can do what's more fun, that is analyzing the simulation behaviour.</p> <pre><code>from AgentTorch.LLM.llm_qa import SimulationAnalysisAgent\nanalyser = SimulationAnalysisAgent(openai_api_key=OPENAI_API_KEY, document_retriever=memory_retriever, temperature=0, state_trace_path=state_trace_path)\n</code></pre> <pre><code>analyser.query(\"Which age group has lowest median income, how much is it?\")\n</code></pre> <pre><code>\u001b[1m&gt; Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `run_analysis_on_simulation_state` with `{'query': 'Which age group has the lowest median income and how much is it?'}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3mThe age group with the lowest median income is 20t29 with a median income of $168.00.\u001b[0m\u001b[32;1m\u001b[1;3mThe age group with the lowest median income is 20-29 years old, with a median income of $168.00.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n{'input': 'Which age group has lowest median income, how much is it?',\n 'output': 'The age group with the lowest median income is 20-29 years old, with a median income of $168.00.'}\n</code></pre> <pre><code>analyser.query(\"Which age group has highest consumption\")\n</code></pre> <pre><code>\u001b[1m&gt; Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `run_analysis_on_simulation_state` with `{'query': 'Which age group has the highest consumption?'}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3mThe age group with the highest consumption is 65A.\u001b[0m\u001b[32;1m\u001b[1;3mThe age group with the highest consumption is 65 and above.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n{'input': 'Which age group has highest consumption',\n 'output': 'The age group with the highest consumption is 65 and above.'}\n</code></pre> <pre><code>analyser.query(\"How is inflation affecting consumption behaviour?\")\n</code></pre> <pre><code>\u001b[1m&gt; Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `simulation_memory_retriever` with `{'query': 'inflation and consumption behaviour'}`\n\n\n\u001b[0m\n\n\nBatches:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\u001b[33;1m\u001b[1;3mReasoning: Let's think step by step in order to\u001b[32m produce the answer. We observe a decrease in the number of COVID cases from 874.0 to 701.0, indicating a potential improvement in the pandemic situation in the Wellington region. This improvement might lead to a higher willingness to work due to perceived lower health risks and possibly more job opportunities as the economy might start to recover. The slight increase in the price of Essential Goods from 12.0 to 12.012574195861816 suggests a minimal inflation in the cost of living, which might not significantly impact the overall expenditure strategy but indicates economic activity. Given the age group (30t39), there's likely a strong incentive to maintain or increase income levels to support any current or future financial goals, such as savings, investments,\u001b[0m\n\nReasoning: Let's think step by step in order to\u001b[32m produce the answer. Given the decrease in the number of COVID cases from 874.0 to 701.0, there is a slight improvement in the pandemic situation in the Waikato region. This improvement might lead to a more stable economic environment and potentially more job security. As a male in the age group of 40 to 49, you are likely to have significant financial responsibilities, possibly including a family to support, a mortgage, and future savings to consider. The slight increase in the price of Essential Goods from 12.0 to 12.012574195861816 indicates a marginal inflation but doesn't significantly impact the overall cost of living. Considering these factors:\n\n1. **Willingness to Work**: With the decrease in\u001b[0m\n\nReasoning: Let's think step by step in order to produce the answer. We observe a decrease in the number of COVID cases from 874.0 to 701.0, indicating a potential improvement in the pandemic situation in the Wellington region. This improvement might lead to a higher willingness to work due to perceived lower health risks and possibly more job opportunities as the economy might start to recover. The slight increase in the price of Essential Goods from 12.0 to 12.012574195861816 suggests a minimal inflation in the cost of living, which might not significantly impact the overall expenditure strategy but indicates economic activity. Given the age group (30t39), there's likely a strong incentive to maintain or increase income levels to support any current or future financial goals, such as savings, investments,\n\nAnswer:\u001b[32m [0.75, 0.6]\u001b[0m\n\nReasoning: Let's think step by step in order to produce the answer. Given the decrease in the number of COVID cases from 874.0 to 701.0, there is a slight improvement in the pandemic situation in the Waikato region. This improvement might lead to a more stable economic environment and potentially more job security. As a male in the age group of 40 to 49, you are likely to have significant financial responsibilities, possibly including a family to support, a mortgage, and future savings to consider. The slight increase in the price of Essential Goods from 12.0 to 12.012574195861816 indicates a marginal inflation but doesn't significantly impact the overall cost of living. Considering these factors: 1. **Willingness to Work**: With the decrease in\n\nAnswer:\u001b[32m [0.7, 0.7]\u001b[0m\n\nReasoning: Let's think step by step in order to produce the answer. We need to consider several factors: 1. **Age and Health Risk**: At age 65, you are in a higher risk category for COVID-19. This increases the risk associated with working, especially if your job cannot be done remotely. 2. **COVID-19 Cases**: The number of cases in the Otago region has decreased from 874.0 to 701.0. This indicates a positive trend, but the risk of infection is still present. 3. **Price of Essential Goods**: The price has slightly increased from 12.0 to 12.012574195861816. This indicates inflation but not at a significant rate that would drastically change consumption habits. 4. **Economic Trends and\n\nAnswer:\u001b[32m [0.25, 0.65]\u001b[0m\n\n\n\n\n\n\n\nYou are an individual living during the COVID-19 pandemic. You need to decide your willingness to work each month and portion of your assests you are willing to spend to meet your consumption demands, based on the current situation of NYC.\n\n---\u001b[0m\u001b[32;1m\u001b[1;3m\nInvoking: `run_analysis_on_simulation_state` with `{'query': 'impact of inflation on consumption behaviour'}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3m          ID    age  area      assets  consumption_propensity ethnicity  \\\n0          0  20t29   0.0    0.000000                    0.00  hispanic\n1          1  20t29   0.0    0.000000                    0.00  hispanic\n2          2  20t29   0.0    0.000000                    0.00     asian\n3          3  20t29   0.0    0.000000                    0.00  hispanic\n4          4  20t29   0.0    0.000000                    0.00  hispanic\n...      ...    ...   ...         ...                     ...       ...\n56775  56775    U19  22.0  333.087189                    1.05     asian\n56776  56776    U19  22.0  221.979263                    1.05     asian\n56777  56777    U19  22.0  221.979263                    1.05  hispanic\n56778  56778    U19  22.0  333.087189                    1.05  hispanic\n56779  56779    U19  22.0  333.087189                    1.05  hispanic\n\n       gender  household  monthly_income  post_tax_income         region  \\\n0        male        0.0        0.000000              0.0  Bay of Plenty\n1        male        1.0        0.000000              0.0  Bay of Plenty\n2      female        2.0        0.000000              0.0  Bay of Plenty\n3        male        3.0        0.000000              0.0  Bay of Plenty\n4        male        4.0        0.000000              0.0  Bay of Plenty\n...       ...        ...             ...              ...            ...\n56775  female      450.0      196.094788              0.0     Wellington\n56776  female      264.0      196.094788              0.0     Wellington\n56777  female      234.0      196.094788              0.0     Wellington\n56778    male      292.0      196.094788              0.0     Wellington\n56779  female      309.0      196.094788              0.0     Wellington\n\n       will_work  month  year  adjusted_consumption  inflation_impact\n0            0.0      0     0                   0.0          0.000000\n1            0.0      0     0                   0.0          0.000000\n2            0.0      0     0                   0.0          0.000000\n3            0.0      0     0                   0.0          0.000000\n4            0.0      0     0                   0.0          0.000000\n...          ...    ...   ...                   ...               ...\n56775        0.0      4     0                   0.0        196.094788\n56776        0.0      4     0                   0.0        196.094788\n56777        0.0      4     0                   0.0        196.094788\n56778        0.0      4     0                   0.0        196.094788\n56779        0.0      4     0                   0.0        196.094788\n\n[283900 rows x 16 columns]\u001b[0m\u001b[32;1m\u001b[1;3mThe analysis of the simulation data shows that inflation has a varying impact on consumption behavior across different demographics. Here are some key observations:\n\n1. For younger age groups (20-29), the impact of inflation on consumption propensity is minimal. This could be due to their relatively lower assets and income levels.\n\n2. For the age group under 19, the impact of inflation is more pronounced. This could be due to their reliance on fixed income sources like allowances or part-time jobs, which do not adjust for inflation.\n\n3. Across different regions, the impact of inflation on consumption behavior also varies. For instance, in the Wellington region, the impact of inflation is more noticeable compared to the Bay of Plenty region.\n\n4. The impact of inflation on consumption behavior also varies by ethnicity and gender. For instance, the Asian and Hispanic communities show a higher impact of inflation on their consumption behavior compared to other ethnicities. Similarly, females show a higher impact of inflation on their consumption behavior compared to males.\n\nThese observations suggest that inflation can affect consumption behavior in complex ways, depending on various demographic and economic factors.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n{'input': 'How is inflation affecting consumption behaviour?',\n 'output': 'The analysis of the simulation data shows that inflation has a varying impact on consumption behavior across different demographics. Here are some key observations:\\n\\n1. For younger age groups (20-29), the impact of inflation on consumption propensity is minimal. This could be due to their relatively lower assets and income levels.\\n\\n2. For the age group under 19, the impact of inflation is more pronounced. This could be due to their reliance on fixed income sources like allowances or part-time jobs, which do not adjust for inflation.\\n\\n3. Across different regions, the impact of inflation on consumption behavior also varies. For instance, in the Wellington region, the impact of inflation is more noticeable compared to the Bay of Plenty region.\\n\\n4. The impact of inflation on consumption behavior also varies by ethnicity and gender. For instance, the Asian and Hispanic communities show a higher impact of inflation on their consumption behavior compared to other ethnicities. Similarly, females show a higher impact of inflation on their consumption behavior compared to males.\\n\\nThese observations suggest that inflation can affect consumption behavior in complex ways, depending on various demographic and economic factors.'}\n</code></pre>"},{"location":"tutorials/using-models/#bring-your-own-population","title":"Bring your own Population","text":"<pre><code>from models import macro_economics\nfrom populations import NYC\nfrom macro_economics.config import NYC_NUM_AGENTS\n\ndata_loader_nyc = DataLoader(model = macro_economics, region = NYC, population_size = NYC_NUM_AGENTS)\nagents_sim = Executor(macro_economics,data_loader_nyc)\nagents_sim.execute()\n</code></pre> <pre><code>Config saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\nConfig saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\nConfig saved at:  /Users/shashankkumar/Documents/GitHub/MacroEcon/models/macro_economics/yamls/config.yaml\nresolvers already registered..\n\nrunning episode 0...\nSubstep Action: Earning decision\nLLM benchmark expts\nSome Example Prompts:  ['You are male of age 20t29, living in the BK region. It is September 2020, number of covid cases is 874.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are male of age 20t29, living in the BX region. It is September 2020, number of covid cases is 874.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are male of age 20t29, living in the MN region. It is September 2020, number of covid cases is 874.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ']\nSubstep: Agent Earning!\nSubstep: Agent Consumption\nExecuting Substep: Labor Market\nExecuting Substep: Financial Market\nSubstep Action: Earning decision\nLLM benchmark expts\nSome Example Prompts:  ['You are male of age 20t29, living in the BK region. It is October 2020, number of covid cases is 701.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are male of age 20t29, living in the BX region. It is October 2020, number of covid cases is 701.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ', 'You are male of age 20t29, living in the MN region. It is October 2020, number of covid cases is 701.0. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your willingness to work this month? Furthermore, how would you plan your expenditures? ']\nSubstep: Agent Earning!\nSubstep: Agent Consumption\nExecuting Substep: Labor Market\nExecuting Substep: Financial Market\nSimulation Ended.\n</code></pre>"}]}